searchData={"items":[{"type":"module","title":"ReqAI","doc":"Universal LLM client built as Req plugins.\n\nProvides multi-provider AI support with cost tracking, token counting,\nand streaming capabilities.","ref":"ReqAI.html"},{"type":"function","title":"ReqAI.attach/2","doc":"Attach ReqAI plugins to a Req request.","ref":"ReqAI.html#attach/2"},{"type":"function","title":"Options - ReqAI.attach/2","doc":"* `:plugins` - List of plugins to attach. Defaults to `[:auth, :token_counter, :cost_tracker]`\n  * `:provider` - AI provider (`:openai`, `:anthropic`, etc.)\n  * `:model` - Model identifier\n  * `:api_key` - API key or key resolution spec\n  * `:stream` - Enable streaming responses (default: false)","ref":"ReqAI.html#attach/2-options"},{"type":"function","title":"Examples - ReqAI.attach/2","doc":"# Basic text generation\n    req = Req.new()\n    |> ReqAI.attach(provider: :openai, model: \"gpt-4\", api_key: \"sk-...\")\n\n    response = Req.post!(req, json: %{\n      messages: [%{role: \"user\", content: \"Hello\"}]\n    })\n\n    # With streaming\n    req = Req.new()\n    |> ReqAI.attach(provider: :openai, model: \"gpt-4\", stream: true)\n\n    # Selective plugins\n    req = Req.new() \n    |> ReqAI.attach(plugins: [:auth, :token_counter], provider: :openai)","ref":"ReqAI.html#attach/2-examples"},{"type":"module","title":"ReqAI.Auth","doc":"Authentication plugin for AI providers.\n\nHandles API key resolution, provider routing, and request authentication.","ref":"ReqAI.Auth.html"},{"type":"function","title":"ReqAI.Auth.attach/2","doc":"","ref":"ReqAI.Auth.html#attach/2"},{"type":"module","title":"ReqAI.CostTracker","doc":"Cost tracking plugin with telemetry integration.\n\nCalculates costs based on usage and emits telemetry events.","ref":"ReqAI.CostTracker.html"},{"type":"function","title":"ReqAI.CostTracker.attach/2","doc":"","ref":"ReqAI.CostTracker.html#attach/2"},{"type":"module","title":"ReqAI.Provider.Anthropic","doc":"","ref":"ReqAI.Provider.Anthropic.html"},{"type":"function","title":"ReqAI.Provider.Anthropic.count_tokens/2","doc":"","ref":"ReqAI.Provider.Anthropic.html#count_tokens/2"},{"type":"function","title":"ReqAI.Provider.Anthropic.name/0","doc":"","ref":"ReqAI.Provider.Anthropic.html#name/0"},{"type":"function","title":"ReqAI.Provider.Anthropic.prepare_request/3","doc":"","ref":"ReqAI.Provider.Anthropic.html#prepare_request/3"},{"type":"module","title":"ReqAI.Provider.OpenAI","doc":"","ref":"ReqAI.Provider.OpenAI.html"},{"type":"function","title":"ReqAI.Provider.OpenAI.count_tokens/2","doc":"","ref":"ReqAI.Provider.OpenAI.html#count_tokens/2"},{"type":"function","title":"ReqAI.Provider.OpenAI.name/0","doc":"","ref":"ReqAI.Provider.OpenAI.html#name/0"},{"type":"function","title":"ReqAI.Provider.OpenAI.prepare_request/3","doc":"","ref":"ReqAI.Provider.OpenAI.html#prepare_request/3"},{"type":"module","title":"ReqAI.Provider.Registry","doc":"Registry for AI provider modules.\n\nThis is a placeholder implementation that will be enhanced in later phases\nto support actual provider implementations.","ref":"ReqAI.Provider.Registry.html"},{"type":"function","title":"ReqAI.Provider.Registry.get/1","doc":"","ref":"ReqAI.Provider.Registry.html#get/1"},{"type":"module","title":"ReqAI.StreamCoalesce","doc":"Optional plugin for streaming response handling.\n\nCoalesces streaming chunks into a unified response format.","ref":"ReqAI.StreamCoalesce.html"},{"type":"function","title":"ReqAI.StreamCoalesce.attach/2","doc":"","ref":"ReqAI.StreamCoalesce.html#attach/2"},{"type":"module","title":"ReqAI.TokenCounter","doc":"Token counting plugin for usage tracking.\n\nEstimates input tokens and tracks actual usage from responses.","ref":"ReqAI.TokenCounter.html"},{"type":"function","title":"ReqAI.TokenCounter.attach/2","doc":"","ref":"ReqAI.TokenCounter.html#attach/2"},{"type":"function","title":"ReqAI.TokenCounter.estimate_tokens/1","doc":"Estimates token count for messages using a simple heuristic.\n\nThis is used as a fallback when provider-specific tokenizers are not available.","ref":"ReqAI.TokenCounter.html#estimate_tokens/1"},{"type":"extras","title":"ReqAI","doc":"# ReqAI\n\nUniversal LLM client built as Req plugins.\n\nProvides multi-provider AI support with cost tracking, token counting,\nand streaming capabilities.","ref":"readme.html"},{"type":"extras","title":"Installation - ReqAI","doc":"If [available in Hex](https://hex.pm/docs/publish), the package can be installed\nby adding `req_ai` to your list of dependencies in `mix.exs`:\n\n```elixir\ndef deps do\n  [\n    {:req_ai, \"~> 0.1.0\"}\n  ]\nend\n```","ref":"readme.html#installation"},{"type":"extras","title":"Usage - ReqAI","doc":"```elixir\n# Attach AI plugins to a Req request\nreq = Req.new() |> ReqAI.attach(provider: :openai, model: \"gpt-4\")\n\n# Make requests with AI-specific features\nresponse = Req.post!(req, json: %{\n  messages: [%{role: \"user\", content: \"Hello!\"}]\n})\n```","ref":"readme.html#usage"},{"type":"extras","title":"Status - ReqAI","doc":"ðŸš§ **Under Development** - This is Phase 0 skeleton setup. Core functionality will be implemented in upcoming phases.","ref":"readme.html#status"},{"type":"extras","title":"Documentation - ReqAI","doc":"Documentation can be generated with [ExDoc](https://github.com/elixir-lang/ex_doc)\nand published on [HexDocs](https://hexdocs.pm). Once published, the docs can\nbe found at  .","ref":"readme.html#documentation"}],"proglang":"elixir","content_type":"text/markdown","producer":{"name":"ex_doc","version":"0.38.3"}}