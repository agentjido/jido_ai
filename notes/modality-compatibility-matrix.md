# Multi-Modal Support Compatibility Matrix

**Generated**: 2025-10-03 09:19:47.563381Z
**Total Models Analyzed**: 299

## Executive Summary

- **Vision-capable models**: 87
- **Audio-capable models**: 25
- **Document-capable models**: 3
- **Multi-modal models** (2+ input modalities): 87

## Validation Results

### Vision Capabilities
- Total vision models: 87
- Known models detected: 4/11
- Detection accuracy: 36.4%
- Status: ⚠️  Below target


### Audio Capabilities
- Total audio models: 25
- Known models detected: 1/2
- Detection accuracy: 50.0%
- Status: ⚠️  Below target


### Document Processing
- Total document-capable models: 3

## Multi-Modal Models

Models supporting 2 or more input modalities:

- **amazon_bedrock:amazon.nova-lite-v1:0**
  - Input: text, image, video
  - Output: text
- **amazon_bedrock:amazon.nova-premier-v1:0**
  - Input: text, image, video
  - Output: text
- **amazon_bedrock:amazon.nova-pro-v1:0**
  - Input: text, image, video
  - Output: text
- **amazon_bedrock:anthropic.claude-3-haiku-20240307-v1:0**
  - Input: text, image
  - Output: text
- **amazon_bedrock:meta.llama3-2-11b-instruct-v1:0**
  - Input: text, image
  - Output: text
- **amazon_bedrock:meta.llama3-2-90b-instruct-v1:0**
  - Input: text, image
  - Output: text
- **amazon_bedrock:meta.llama4-maverick-17b-instruct-v1:0**
  - Input: text, image
  - Output: text
- **amazon_bedrock:meta.llama4-scout-17b-instruct-v1:0**
  - Input: text, image
  - Output: text
- **anthropic:claude-3-haiku-20240307**
  - Input: text, image
  - Output: text
- **azure:gpt-4.1-mini**
  - Input: text, image
  - Output: text
- **azure:gpt-4.1-nano**
  - Input: text, image
  - Output: text
- **azure:gpt-4o-mini**
  - Input: text, image
  - Output: text
- **azure:gpt-5-nano**
  - Input: text, image
  - Output: text
- **azure:o4-mini**
  - Input: text, image
  - Output: text
- **chutes:moonshotai/Kimi-VL-A3B-Thinking**
  - Input: text, image
  - Output: text
- **fastrouter:google/gemini-2.5-flash**
  - Input: text, image, pdf
  - Output: text
- **fastrouter:openai/gpt-5-nano**
  - Input: text, image
  - Output: text
- **github_copilot:claude-3.5-sonnet**
  - Input: text, image
  - Output: text
- **github_copilot:claude-3.7-sonnet**
  - Input: text, image
  - Output: text
- **github_copilot:claude-3.7-sonnet-thought**
  - Input: text, image
  - Output: text
- **github_copilot:claude-opus-4**
  - Input: text, image
  - Output: text
- **github_copilot:claude-opus-41**
  - Input: text, image
  - Output: text
- **github_copilot:claude-sonnet-4**
  - Input: text, image
  - Output: text
- **github_copilot:gemini-2.0-flash-001**
  - Input: text, image, audio, video
  - Output: text
- **github_copilot:gemini-2.5-pro**
  - Input: text, image, audio, video
  - Output: text
- **github_copilot:gpt-4.1**
  - Input: text, image
  - Output: text
- **github_copilot:gpt-4o**
  - Input: text, image
  - Output: text
- **github_copilot:gpt-5**
  - Input: text, image
  - Output: text
- **github_copilot:gpt-5-mini**
  - Input: text, image
  - Output: text
- **github_copilot:o3**
  - Input: text, image
  - Output: text
- **google:gemini-1.5-flash**
  - Input: text, image, audio, video
  - Output: text
- **google:gemini-1.5-flash-8b**
  - Input: text, image, audio, video
  - Output: text
- **google:gemini-2.0-flash**
  - Input: text, image, audio, video, pdf
  - Output: text
- **google:gemini-2.0-flash-lite**
  - Input: text, image, audio, video, pdf
  - Output: text
- **google:gemini-2.5-flash**
  - Input: text, image, audio, video, pdf
  - Output: text
- **google:gemini-2.5-flash-lite-preview-06-17**
  - Input: text, image, audio, video, pdf
  - Output: text
- **google:gemini-2.5-flash-preview-04-17**
  - Input: text, image, audio, video, pdf
  - Output: text
- **google:gemini-2.5-flash-preview-05-20**
  - Input: text, image, audio, video, pdf
  - Output: text
- **google_vertex:gemini-2.0-flash**
  - Input: text, image, audio, video, pdf
  - Output: text
- **google_vertex:gemini-2.0-flash-lite**
  - Input: text, image, audio, video, pdf
  - Output: text
- **google_vertex:gemini-2.5-flash**
  - Input: text, image, audio, video, pdf
  - Output: text
- **google_vertex:gemini-2.5-flash-lite-preview-06-17**
  - Input: text, image, audio, video, pdf
  - Output: text
- **google_vertex:gemini-2.5-flash-preview-04-17**
  - Input: text, image, audio, video, pdf
  - Output: text
- **google_vertex:gemini-2.5-flash-preview-05-20**
  - Input: text, image, audio, video, pdf
  - Output: text
- **groq:meta-llama/llama-4-maverick-17b-128e-instruct**
  - Input: text, image
  - Output: text
- **groq:meta-llama/llama-4-scout-17b-16e-instruct**
  - Input: text, image
  - Output: text
- **groq:meta-llama/llama-guard-4-12b**
  - Input: text, image
  - Output: text
- **inference:google/gemma-3**
  - Input: text, image
  - Output: text
- **inference:meta/llama-3.2-11b-vision-instruct**
  - Input: text, image
  - Output: text
- **inference:qwen/qwen-2.5-7b-vision-instruct**
  - Input: text, image
  - Output: text
- **mistral:mistral-small-latest**
  - Input: text, image
  - Output: text
- **mistral:pixtral-12b**
  - Input: text, image
  - Output: text
- **openai:gpt-4.1-mini**
  - Input: text, image
  - Output: text
- **openai:gpt-4.1-nano**
  - Input: text, image
  - Output: text
- **openai:gpt-4o-mini**
  - Input: text, image
  - Output: text
- **openai:gpt-5-nano**
  - Input: text, image
  - Output: text
- **openai:o4-mini**
  - Input: text, image
  - Output: text
- **openrouter:google/gemini-2.0-flash-001**
  - Input: text, image, audio, video, pdf
  - Output: text
- **openrouter:google/gemini-2.5-flash**
  - Input: text, image, audio, video, pdf
  - Output: text
- **openrouter:openai/gpt-4.1-mini**
  - Input: text, image
  - Output: text
- **openrouter:openai/gpt-4o-mini**
  - Input: text, image
  - Output: text
- **openrouter:openai/gpt-5-nano**
  - Input: text, image
  - Output: text
- **openrouter:openai/o4-mini**
  - Input: text, image
  - Output: text
- **openrouter:z-ai/glm-4.5v**
  - Input: text, image, video
  - Output: text
- **requesty:google/gemini-2.5-flash**
  - Input: text, image, audio, video, pdf
  - Output: text
- **requesty:openai/gpt-4.1-mini**
  - Input: text, image
  - Output: text
- **requesty:openai/gpt-4o-mini**
  - Input: text, image
  - Output: text
- **requesty:openai/o4-mini**
  - Input: text, image
  - Output: text
- **synthetic:hf:meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8**
  - Input: text, image
  - Output: text
- **synthetic:hf:meta-llama/Llama-4-Scout-17B-16E-Instruct**
  - Input: text, image
  - Output: text
- **venice:qwen-2.5-vl**
  - Input: text, image
  - Output: text
- **vercel:amazon/nova-lite**
  - Input: text, image, video
  - Output: text
- **vercel:amazon/nova-pro**
  - Input: text, image, video
  - Output: text
- **vercel:anthropic/claude-3-haiku**
  - Input: text, image
  - Output: text
- **vercel:google/gemini-2.0-flash**
  - Input: text, image, audio, video, pdf
  - Output: text
- **vercel:google/gemini-2.0-flash-lite**
  - Input: text, image, audio, video, pdf
  - Output: text
- **vercel:google/gemini-2.5-flash**
  - Input: text, image, audio, video, pdf
  - Output: text
- **vercel:mistral/mistral-small**
  - Input: text, image
  - Output: text
- **vercel:mistral/pixtral-12b**
  - Input: text, image
  - Output: text
- **vercel:openai/gpt-4.1-mini**
  - Input: text, image
  - Output: text
- **vercel:openai/gpt-4.1-nano**
  - Input: text, image
  - Output: text
- **vercel:openai/gpt-4o-mini**
  - Input: text, image
  - Output: text
- **vercel:openai/gpt-5-nano**
  - Input: text, image
  - Output: text
- **vercel:openai/o4-mini**
  - Input: text, image
  - Output: text
- **wandb:meta-llama/Llama-4-Scout-17B-16E-Instruct**
  - Input: text, image
  - Output: text
- **zai:glm-4.5v**
  - Input: text, image, video
  - Output: text
- **zhipuai:glm-4.5v**
  - Input: text, image, video
  - Output: text

## Provider Statistics

| Provider | Total Models | Vision | Audio | Multi-Modal |
|----------|--------------|--------|-------|-------------|
| amazon_bedrock | 21 | 8 | 0 | 8 |
| anthropic | 1 | 1 | 0 | 1 |
| azure | 8 | 5 | 0 | 5 |
| baseten | 2 | 0 | 0 | 0 |
| cerebras | 2 | 0 | 0 | 0 |
| chutes | 20 | 1 | 0 | 1 |
| cloudflare_workers_ai | 29 | 0 | 3 | 0 |
| deepinfra | 3 | 0 | 0 | 0 |
| deepseek | 2 | 0 | 0 | 0 |
| fastrouter | 7 | 2 | 0 | 2 |
| fireworks_ai | 8 | 0 | 0 | 0 |
| github_copilot | 16 | 13 | 2 | 13 |
| google | 8 | 8 | 8 | 8 |
| google_vertex | 6 | 6 | 6 | 6 |
| groq | 15 | 3 | 0 | 3 |
| huggingface | 3 | 0 | 0 | 0 |
| inference | 8 | 3 | 0 | 3 |
| mistral | 11 | 2 | 0 | 2 |
| moonshotai | 2 | 0 | 0 | 0 |
| moonshotai_cn | 2 | 0 | 0 | 0 |
| morph | 3 | 0 | 0 | 0 |
| openai | 11 | 5 | 0 | 5 |
| opencode | 2 | 0 | 0 | 0 |
| openrouter | 26 | 7 | 2 | 7 |
| requesty | 5 | 4 | 1 | 4 |
| submodel | 9 | 0 | 0 | 0 |
| synthetic | 15 | 2 | 0 | 2 |
| togetherai | 3 | 0 | 0 | 0 |
| upstage | 2 | 0 | 0 | 0 |
| venice | 5 | 1 | 0 | 1 |
| vercel | 26 | 13 | 3 | 13 |
| wandb | 8 | 1 | 0 | 1 |
| xai | 4 | 0 | 0 | 0 |
| zai | 3 | 1 | 0 | 1 |
| zhipuai | 3 | 1 | 0 | 1 |

## Modality Distribution

- **input audio**: 25 models
- **input image**: 87 models
- **input pdf**: 19 models
- **input text**: 296 models
- **input video**: 30 models
- **output audio**: 1 models
- **output embedding**: 3 models
- **output image**: 3 models
- **output text**: 292 models

## Notes

This matrix was generated by validating modality metadata from the Model Registry.
Accuracy is calculated based on known vision and audio models from major providers.

**Prepared for**: Phase 3 Multi-Modal Implementation
