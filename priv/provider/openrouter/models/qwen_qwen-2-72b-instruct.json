{
  "data": {
    "architecture": {
      "instruct_type": "chatml",
      "modality": "text->text",
      "tokenizer": "Qwen"
    },
    "created": 1717718400,
    "description": "Qwen2 72B is a transformer-based model that excels in language understanding, multilingual capabilities, coding, mathematics, and reasoning.\n\nIt features SwiGLU activation, attention QKV bias, and group query attention. It is pretrained on extensive data with supervised finetuning and direct preference optimization.\n\nFor more details, see this [blog post](https://qwenlm.github.io/blog/qwen2/) and [GitHub repo](https://github.com/QwenLM/Qwen2).\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).",
    "endpoints": [
      {
        "context_length": 32768,
        "max_completion_tokens": 4096,
        "max_prompt_tokens": null,
        "name": "Together | qwen/qwen-2-72b-instruct",
        "pricing": {
          "completion": "0.0000009",
          "image": "0",
          "prompt": "0.0000009",
          "request": "0"
        },
        "provider_name": "Together",
        "quantization": "unknown",
        "supported_parameters": [
          "max_tokens",
          "temperature",
          "top_p",
          "stop",
          "frequency_penalty",
          "presence_penalty",
          "top_k",
          "repetition_penalty",
          "logit_bias",
          "min_p",
          "response_format"
        ]
      }
    ],
    "id": "qwen/qwen-2-72b-instruct",
    "name": "Qwen 2 72B Instruct"
  }
}