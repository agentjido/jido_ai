{
  "data": {
    "architecture": {
      "instruct_type": null,
      "modality": "text->text",
      "tokenizer": "Qwen"
    },
    "created": 1738402289,
    "description": "Qwen-Max, based on Qwen2.5, provides the best inference performance among [Qwen models](/qwen), especially for complex multi-step tasks. It's a large-scale MoE model that has been pretrained on over 20 trillion tokens and further post-trained with curated Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) methodologies. The parameter count is unknown.",
    "endpoints": [
      {
        "context_length": 32768,
        "max_completion_tokens": 8192,
        "max_prompt_tokens": 30720,
        "name": "Alibaba | qwen/qwen-max-2025-01-25",
        "pricing": {
          "completion": "0.0000064",
          "image": "0",
          "prompt": "0.0000016",
          "request": "0"
        },
        "provider_name": "Alibaba",
        "quantization": null,
        "supported_parameters": [
          "tools",
          "tool_choice",
          "max_tokens",
          "temperature",
          "top_p",
          "seed",
          "response_format",
          "presence_penalty"
        ]
      }
    ],
    "id": "qwen/qwen-max",
    "name": "Qwen: Qwen-Max "
  }
}