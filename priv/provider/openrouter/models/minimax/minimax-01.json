{
  "id": "minimax/minimax-01",
  "name": "MiniMax: MiniMax-01",
  "description": "MiniMax-01 is a combines MiniMax-Text-01 for text generation and MiniMax-VL-01 for image understanding. It has 456 billion parameters, with 45.9 billion parameters activated per inference, and can handle a context of up to 4 million tokens.\n\nThe text model adopts a hybrid architecture that combines Lightning Attention, Softmax Attention, and Mixture-of-Experts (MoE). The image model adopts the “ViT-MLP-LLM” framework and is trained on top of the text model.\n\nTo read more about the release, see: https://www.minimaxi.com/en/news/minimax-01-series-2",
  "capabilities": {
    "code": false,
    "image": true,
    "chat": true,
    "embedding": false,
    "vision": true,
    "multimodal": true,
    "audio": false
  },
  "tier": {
    "value": "basic",
    "description": "Entry-level model"
  },
  "architecture": {
    "modality": "text+image->text",
    "tokenizer": "Other",
    "instruct_type": null
  },
  "created": 1736915462,
  "endpoints": []
}