{
  "data": {
    "architecture": {
      "instruct_type": "llama3",
      "modality": "text->text",
      "tokenizer": "Llama3"
    },
    "created": 1727222400,
    "description": "Llama 3.2 1B is a 1-billion-parameter language model focused on efficiently performing natural language tasks, such as summarization, dialogue, and multilingual text analysis. Its smaller size allows it to operate efficiently in low-resource environments while maintaining strong task performance.\n\nSupporting eight core languages and fine-tunable for more, Llama 1.3B is ideal for businesses or developers seeking lightweight yet powerful AI solutions that can operate in diverse multilingual settings without the high computational demand of larger models.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).",
    "endpoints": [
      {
        "context_length": 131072,
        "max_completion_tokens": null,
        "max_prompt_tokens": null,
        "name": "Lepton | meta-llama/llama-3.2-1b-instruct",
        "pricing": {
          "completion": "0.00000001",
          "image": "0",
          "prompt": "0.00000001",
          "request": "0"
        },
        "provider_name": "Lepton",
        "quantization": "bf16",
        "supported_parameters": [
          "max_tokens",
          "temperature",
          "top_p",
          "stop",
          "frequency_penalty",
          "presence_penalty",
          "repetition_penalty",
          "logit_bias",
          "top_k",
          "min_p",
          "seed"
        ]
      },
      {
        "context_length": 131072,
        "max_completion_tokens": 8192,
        "max_prompt_tokens": null,
        "name": "DeepInfra | meta-llama/llama-3.2-1b-instruct",
        "pricing": {
          "completion": "0.00000001",
          "image": "0",
          "prompt": "0.00000001",
          "request": "0"
        },
        "provider_name": "DeepInfra",
        "quantization": "bf16",
        "supported_parameters": [
          "max_tokens",
          "temperature",
          "top_p",
          "stop",
          "frequency_penalty",
          "presence_penalty",
          "repetition_penalty",
          "response_format",
          "top_k",
          "seed",
          "min_p"
        ]
      },
      {
        "context_length": 4096,
        "max_completion_tokens": 2048,
        "max_prompt_tokens": 2048,
        "name": "SambaNova | meta-llama/llama-3.2-1b-instruct",
        "pricing": {
          "completion": "0.00000008",
          "image": "0",
          "prompt": "0.00000004",
          "request": "0"
        },
        "provider_name": "SambaNova",
        "quantization": "bf16",
        "supported_parameters": [
          "max_tokens",
          "temperature",
          "top_p",
          "top_k",
          "stop"
        ]
      },
      {
        "context_length": 131072,
        "max_completion_tokens": null,
        "max_prompt_tokens": 131072,
        "name": "Cloudflare | meta-llama/llama-3.2-1b-instruct",
        "pricing": {
          "completion": "0.0000001",
          "image": "0",
          "prompt": "0.0000001",
          "request": "0"
        },
        "provider_name": "Cloudflare",
        "quantization": "unknown",
        "supported_parameters": [
          "max_tokens",
          "temperature",
          "top_p",
          "top_k",
          "seed",
          "repetition_penalty",
          "frequency_penalty",
          "presence_penalty"
        ]
      }
    ],
    "id": "meta-llama/llama-3.2-1b-instruct",
    "name": "Meta: Llama 3.2 1B Instruct"
  }
}