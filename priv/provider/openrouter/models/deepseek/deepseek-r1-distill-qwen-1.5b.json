{
  "id": "deepseek/deepseek-r1-distill-qwen-1.5b",
  "name": "DeepSeek: R1 Distill Qwen 1.5B",
  "description": "DeepSeek R1 Distill Qwen 1.5B is a distilled large language model based on  [Qwen 2.5 Math 1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). It's a very small and efficient model which outperforms [GPT 4o 0513](/openai/gpt-4o-2024-05-13) on Math Benchmarks.\n\nOther benchmark results include:\n\n- AIME 2024 pass@1: 28.9\n- AIME 2024 cons@64: 52.7\n- MATH-500 pass@1: 83.9\n\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.",
  "capabilities": {
    "code": false,
    "image": false,
    "chat": true,
    "embedding": false,
    "vision": false,
    "multimodal": false,
    "audio": false
  },
  "tier": {
    "value": "basic",
    "description": "Entry-level model"
  },
  "architecture": {
    "modality": "text->text",
    "tokenizer": "Other",
    "instruct_type": "deepseek-r1"
  },
  "created": 1738328067,
  "endpoints": []
}