{
  "id": "mistralai/ministral-8b",
  "name": "Mistral: Ministral 8B",
  "description": "Ministral 8B is an 8B parameter model featuring a unique interleaved sliding-window attention pattern for faster, memory-efficient inference. Designed for edge use cases, it supports up to 128k context length and excels in knowledge and reasoning tasks. It outperforms peers in the sub-10B category, making it perfect for low-latency, privacy-first applications.",
  "capabilities": {
    "code": false,
    "image": false,
    "chat": true,
    "embedding": false,
    "vision": false,
    "multimodal": false,
    "audio": false
  },
  "tier": {
    "value": "basic",
    "description": "Entry-level model"
  },
  "architecture": {
    "modality": "text->text",
    "tokenizer": "Mistral",
    "instruct_type": null
  },
  "created": 1729123200,
  "endpoints": []
}